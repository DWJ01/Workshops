{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow: The Estimator API\n",
    "\n",
    "The Estimators are a high-level TensorFlow API, making your machine learning model making much easier. With the Estimator API you can train, test, and predict datapoints. There are a couple of pre-made estimators such as ```LinearRegressor```. For this workshop we are going to use this high-level API to practice the fundamentals of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noelkonagai/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the data\n",
    "\n",
    "In order to complete this exercise, please create your artificial data in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = np.linspace(0.0, 10.0, 1000000)\n",
    "noise = np.random.randn(len(x_data))\n",
    "b = 5\n",
    "\n",
    "y_true = (0.5 * x_data ) + b + noise\n",
    "\n",
    "my_data = pd.concat([pd.DataFrame(data=x_data,columns=['x']),pd.DataFrame(data=y_true,columns=['y'])],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up the tf.estimator API\n",
    "\n",
    "The ```tf.estimator.LinearRegressor``` takes in feature columns as its argument. For this you need to create a feature column with ```tf.feature_column.numeric_column```. There are other types of columns as well. You can read more through the official API guide [tf.feature_column](https://www.tensorflow.org/api_docs/python/tf/feature_column).\n",
    "\n",
    "** What is a feature column? ** \n",
    "\n",
    "The data used for machine learning typically consists of features and a label. Say, in a dataset of housing listings, the features would be the number of rooms, the floor area, the district it is located in, the year the house was built or renovated, and more. The label would be the price of that apartment. In our case, we only have a simple line y = mx + b. We only have one feature and that is \"x\". \n",
    "\n",
    "The remaining types of feature columns we will see later on. For now, since our feature \"x\" is continuous, we use ```numeric_column```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/j5/j768nzj554jbgl0ypv8vxk640000gp/T/tmpeetcqwii\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/j5/j768nzj554jbgl0ypv8vxk640000gp/T/tmpeetcqwii', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11448de80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "feat_cols = [tf.feature_column.numeric_column('x',shape=[1])]\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test split of data\n",
    "\n",
    "Machine learning typically involves splitting the data into three parts. The train data is the dataset on which you train your model. Test data is the data on which you... test your data. But there is a third one, we won't be using it today. It is called evaluate data. It's the last dataset which your model has not ever seen, never been trained on. After you fine-tuned your model, you can typically recombine the train and test data, train your model with it and make a final evaluation with your eval data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(x_data,y_true,test_size=0.3, random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating input functions\n",
    "\n",
    "The training instance of the estimator API takes in an input function that you can call with [tf.estimator.inputs.numpy_input_fn](https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn).\n",
    "\n",
    "For the purposes of this workshop we won't worry about queue capacity and number of threads that could be input in the function below. We will only input the feature column, the label, batch size and the number of epochs.\n",
    "\n",
    "```python\n",
    "numpy_input_fn(\n",
    "    x,\n",
    "    y=None,\n",
    "    batch_size=128,\n",
    "    num_epochs=1,\n",
    "    shuffle=None,\n",
    "    queue_capacity=1000,\n",
    "    num_threads=1\n",
    ")\n",
    "```\n",
    "\n",
    "** What is a batch, an epoch, and how do I choose this values? **\n",
    "\n",
    "These are concepts that have to do with the input of the data. Let's start with an epoch. One **epoch** is when an entire dataset is passed through the function. In case of neural networks, there is something caled forward and backward propagation. When you pass in the data through the neural network in both directions, it counts as one epoch. Typically, passing in all the data at once is too tolling for our computer's performance so we select smaller portions, in other words **batches**. \n",
    "\n",
    "The **batch size** is the total number of training examples present in a batch. The higher the batch size, the more representative of the whole dataset and the more time it takes to train our model. So why is one epoch not enough to train our data? If you trained your model, feeding in batches only once, then you are likely **underfitting** the model to your data. However, if you increase your number of epochs to too large, you're risking **overfitting**. What this means is that you're tailoring the model too much to the data that you are feeding into your model. \n",
    "\n",
    "There are some tools that help you fight overfitting. For instance regularization methods, but they are beyond the scope of this workshop. Another is **shuffle**. Shuffle means switching up the order of the data that was fed in. If you turn shuffle off after each epoch, you are likely creating batches that are not representative of the overall dataset, and hence you will get skewed predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_func = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'x': x_train},\n",
    "    y_train,\n",
    "    batch_size=4,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "\n",
    "test_input_func = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'x': x_eval},\n",
    "    y_eval,\n",
    "    batch_size=4,\n",
    "    num_epochs=1000,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.estimator.LinearRegressor.train\n",
    "\n",
    "The ```estimator.train``` function takes in an input function that we have defined in the above cell and the number of steps to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/j5/j768nzj554jbgl0ypv8vxk640000gp/T/tmpeetcqwii/model.ckpt.\n",
      "INFO:tensorflow:loss = 287.38535, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1030.52\n",
      "INFO:tensorflow:loss = 21.260845, step = 101 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 1227.47\n",
      "INFO:tensorflow:loss = 2.1927717, step = 201 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1130.22\n",
      "INFO:tensorflow:loss = 9.52873, step = 301 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 1026.94\n",
      "INFO:tensorflow:loss = 11.264387, step = 401 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 1044.56\n",
      "INFO:tensorflow:loss = 4.286193, step = 501 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1031.5\n",
      "INFO:tensorflow:loss = 5.714856, step = 601 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 1058.65\n",
      "INFO:tensorflow:loss = 7.6289186, step = 701 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 996.849\n",
      "INFO:tensorflow:loss = 4.224108, step = 801 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 998.463\n",
      "INFO:tensorflow:loss = 4.698669, step = 901 (0.101 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/j5/j768nzj554jbgl0ypv8vxk640000gp/T/tmpeetcqwii/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.158762.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearRegressor at 0x1108f97b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.train(input_fn = train_input_func, steps = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see how our model did on the test/eval dataset, we run ```estimator.evaluate```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-08-05:57:49\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/j5/j768nzj554jbgl0ypv8vxk640000gp/T/tmpeetcqwii/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [100/1000]\n",
      "INFO:tensorflow:Evaluation [200/1000]\n",
      "INFO:tensorflow:Evaluation [300/1000]\n",
      "INFO:tensorflow:Evaluation [400/1000]\n",
      "INFO:tensorflow:Evaluation [500/1000]\n",
      "INFO:tensorflow:Evaluation [600/1000]\n",
      "INFO:tensorflow:Evaluation [700/1000]\n",
      "INFO:tensorflow:Evaluation [800/1000]\n",
      "INFO:tensorflow:Evaluation [900/1000]\n",
      "INFO:tensorflow:Evaluation [1000/1000]\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-08-05:57:50\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 1.1569067, global_step = 1000, loss = 4.627627\n"
     ]
    }
   ],
   "source": [
    "test_metrics = estimator.evaluate(input_fn = test_input_func,steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test metrics: {'average_loss': 1.1569067, 'loss': 4.627627, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "print(\"test metrics: {}\".format(test_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a predict function, where we feed in a new x dataset using ```np.linspace(0,10,10)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_input_func = tf.estimator.inputs.numpy_input_fn({'x':np.linspace(0,10,10)},shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Estimator.predict at 0x129417888>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.predict(input_fn = predict_input_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the predictions into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_fn_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-205f24e5fc18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m# np.array([])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_fn_predict' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = []# np.array([])\n",
    "for x in estimator.predict(input_fn = predict_input_func):\n",
    "    predictions.append(x['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.sample(n=250).plot(kind='scatter',x='x',y='y')\n",
    "plt.plot(np.linspace(0,10,10),predictions,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
